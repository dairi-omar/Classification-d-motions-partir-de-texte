<h1 style="color: red;  text-align: center;">Classification d‚Äô√©motions √† partir de texte</h1>

## üéØ Objectif

Ce projet vise √† construire et √©valuer des mod√®les de deep learning (RNN) pour la reconnaissance et la classification des √©motions √† partir de textes courts, tels que les Tweets et les posts sur les r√©seaux sociaux.

Le mod√®le est entra√Æn√© √† reconna√Ætre six √©motions principales :
* Joie (joy)
* Tristesse (sadness)
* Col√®re (anger)
* Peur (fear)
* Amour (love)
* Surprise (surprise)

## üíæ Dataset

Nous utilisons le "Emotions Dataset for NLP" disponible sur Kaggle. Le notebook utilise la r√©partition suivante :
* **Entra√Ænement :** 16 000 exemples (`train.txt`)
* **Test / Validation :** 2 000 exemples (`test.txt`)

## üõ†Ô∏è Pipeline de Traitement

1.  **Chargement des donn√©es :** Les fichiers `.txt` sont charg√©s dans des DataFrames Pandas, s√©par√©s par `;`.
2.  **Nettoyage du texte :** Une fonction `clean_text` est appliqu√©e pour :
    * Mettre le texte en minuscule.
    * Supprimer tous les caract√®res non alphab√©tiques (ponctuation, chiffres) via regex.
    * Supprimer les espaces superflus.
3.  **Encodage des √©tiquettes :** Les 6 √©tiquettes textuelles (joie, tristesse, etc.) sont transform√©es en vecteurs one-hot en utilisant `LabelEncoder` de Scikit-learn et `to_categorical` de Keras.
4.  **Tokenisation et Padding :**
    * Le texte est tokenis√© avec le `Tokenizer` de Keras (vocabulaire limit√© √† 10 000 mots).
    * Les phrases sont converties en s√©quences d'entiers.
    * Les s√©quences sont normalis√©es √† une longueur fixe de **50 tokens** via `pad_sequences`.

## üß† Mod√®les et R√©sultats

Deux architectures de r√©seaux de neurones r√©currents ont √©t√© test√©es :

### 1. Mod√®le LSTM
* **Architecture :** `Embedding(10000, 128)` -> `LSTM(128, return_seq=True)` -> `LSTM(64)` -> `Dense(64, 'relu')` -> `Dropout(0.5)` -> `Dense(6, 'softmax')`.
* **Performance (Test) :** **89.25%** d'accuracy.

### 2. Mod√®le GRU (Gated Recurrent Unit)
* **Architecture :** `Embedding(10000, 128)` -> `GRU(128, return_seq=True)` -> `GRU(64)` -> `Dense(64, 'relu')` -> `Dropout(0.5)` -> `Dense(6, 'softmax')`.
* **Performance (Test) :** **91.35%** d'accuracy.

### Conclusion
Le mod√®le **GRU** s'est montr√© plus performant que le mod√®le LSTM pour cette t√¢che de classification, avec une pr√©cision de 91.35% sur l'ensemble de test.

## üíæ Mod√®le Sauvegard√©

Le mod√®le GRU (le plus performant) est sauvegard√© dans le fichier `EmotionsFromText_model_gru.h5`.

## ‚öôÔ∏è Comment l'utiliser

1.  Clonez ce d√©p√¥t.
2.  Assurez-vous d'avoir le dataset (`train.txt`, `test.txt`) de Kaggle.
3.  **Important :** Mettez √† jour la variable `datasetPath` dans la 5√®me cellule de code pour pointer vers l'emplacement de votre dossier `DataSet`. Le notebook utilise un chemin absolu qui doit √™tre modifi√© :
    ```python
    # Cellule 5
    datasetPath = r"D:\Projects\EmotionsFromText\DataSet" 
    # (√Ä CHANGER)
    ```
4.  Installez les d√©pendances requises (voir `requirements.txt`) :
    ```bash
    pip install pandas seaborn matplotlib scikit-learn tensorflow
    ```
5.  Ex√©cutez le notebook `EmotionsFromText.ipynb`.
