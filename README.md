<h1 style="color: red;  text-align: center;">Classification d‚Äô√©motions √† partir de texte</h1>

## üéØ Objectif

Ce projet vise √† construire un mod√®le de deep learning pour la reconnaissance et la classification des √©motions √† partir de textes courts, tels que les Tweets et les posts sur les r√©seaux sociaux.

Le mod√®le est entra√Æn√© √† reconna√Ætre six √©motions principales :
* Joie (joy)
* Tristesse (sadness)
* Col√®re (anger)
* Peur (fear)
* Amour (love)
* Surprise (surprise)

## üíæ Dataset

Nous utilisons le "Emotions Dataset for NLP" disponible sur Kaggle. Le notebook utilise la r√©partition suivante :
* **Entra√Ænement :** 16 000 exemples (`train.txt`)
* **Test / Validation :** 2 000 exemples (`test.txt`)

## üõ†Ô∏è Pipeline de Traitement

1.  **Chargement des donn√©es :** Les fichiers `.txt` sont charg√©s dans des DataFrames Pandas.
2.  **Nettoyage du texte :** Une fonction `clean_text` est appliqu√©e pour :
    * Mettre le texte en minuscule.
    * Supprimer tous les caract√®res non alphab√©tiques (ponctuation, chiffres) via regex.
    * Supprimer les espaces superflus.
3.  **Encodage des √©tiquettes :** Les 6 √©tiquettes textuelles (joie, tristesse, etc.) sont transform√©es en vecteurs one-hot en utilisant `LabelEncoder` de Scikit-learn et `to_categorical` de Keras.
4.  **Tokenisation et Padding :**
    * Le texte est tokenis√© avec le `Tokenizer` de Keras (vocabulaire limit√© √† 10 000 mots).
    * Les phrases sont converties en s√©quences d'entiers.
    * Les s√©quences sont normalis√©es √† une longueur fixe de **50 tokens** via `pad_sequences`.

## üß† Architecture du Mod√®le

Le mod√®le est un `Sequential` de Keras bas√© sur des couches LSTM bidirectionnelles :

1.  **Embedding** : (input_dim=10000, output_dim=128, input_length=50)
2.  **LSTM(128)** (avec dropout de 0.3)
3.  **LSTM(64)** (avec dropout de 0.3)
4.  **Dense** (64 neurones, activation 'relu')
5.  **Dropout** (0.5)
6.  **Dense** (6 neurones, activation 'softmax') - Couche de sortie pour les 6 classes.

## üöÄ Entra√Ænement et R√©sultats

* **Optimiseur :** Adam
* **Fonction de perte :** `categorical_crossentropy`
* **M√©triques :** Accuracy
* **√âpoques :** 10
* **Taille de lot (Batch Size) :** 256

Le mod√®le atteint une **pr√©cision (accuracy) de 90.45%** sur l'ensemble de test. Les courbes ROC et les scores AUC pour chaque classe confirment d'excellentes performances (AUC > 0.96 pour toutes les classes).

## ‚öôÔ∏è Comment l'utiliser

1.  Clonez ce d√©p√¥t.
2.  Assurez-vous d'avoir le dataset (`train.txt`, `test.txt`) disponible.
3.  **Important :** Mettez √† jour la variable `datasetPath` dans la 5√®me cellule de code pour pointer vers l'emplacement de votre dossier `DataSet`. Le notebook utilise un chemin absolu :
    ```python
    datasetPath = r"D:\Projects\EmotionsFromText\DataSet" 
    # (√Ä CHANGER)
    ```
4.  Installez les d√©pendances requises :
    ```bash
    pip install -r requirements.txt
    ```
5.  Ex√©cutez le notebook `EmotionsFromText.ipynb` (par exemple, avec Jupyter Lab ou VS Code).
